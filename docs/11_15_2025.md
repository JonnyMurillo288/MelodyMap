# Product/Design/Review (PDR) — SixDegreesSpotify

Date: 2025-11-15
Scope: Backend search logic, Spotify API integration, HTTP handlers, and templates overview in current repository state.


## 1) What the code does (architecture and flow)

- Goal: Given two Spotify entities (artists/tracks), compute a connection path via collaborations/graph relationships ("six degrees"), show it on web pages, and support recommendations/playing features.

- Core packages:
  - sixDegrees/ — Graph and search domain logic
    - artists.go, tracks.go: Define domain types and parsing helpers around Spotify entities, construct edges among tracks/artists, and provide search glue.
    - weightedSearch.go, weighted_strategy_test.go: Weighted search strategy and Dijkstra variants for pathfinding with costs.
    - shortestPath_test.go, weightedSearch_test.go, search_test.go: Test coverage for search behavior.
  - Root-level algorithms: bfs.go, dijkstras.go, shared_search_funcs.go, shared.go: Implement BFS and Dijkstra/weighted search over the collaboration graph using abstractions shared with the sixDegrees package.
  - spotify/spotify.go: Encapsulates Spotify Web API calls (search, get artists/tracks, audio features, currently playing, etc.). Handles pagination/limits and returns typed data used by domain logic.

- HTTP layer (main/):
  - main.go: Binds routes and starts the server.
  - http_search.go: Handlers for search forms and path results. Accepts inputs (source/target, algorithm choice), invokes search (BFS/weighted/Dijkstra), and renders templates or JSON.
  - auth.go: Spotify OAuth authorization code flow handlers, token retrieval and refresh, stores tokens in store.go.
  - store.go: Manages user/session/token state (in-memory or file-backed) used by handlers to authenticate Spotify API calls.

- Templates (templates/):
  - index.html, path_form.html, path_result.html, graph.html, loading.html, etc.: Views to present the search UI, a loading screen during computation, and visualize the resulting path/graph.

- CLI/docs:
  - docs/PDR.md, PDR_frontend.md and several flow docs explain earlier flows and design decisions.

- End-to-end flow:
  1) User authenticates with Spotify via OAuth (main/auth.go) and tokens are stored (store.go).
  2) User submits a search from the UI (templates/index.html -> http_search.go).
  3) Backend builds a graph implicitly by querying Spotify for artists/tracks and relations (spotify/spotify.go) and traverses it with BFS or weighted search (bfs.go/dijkstras.go or sixDegrees/weightedSearch.go).
  4) The discovered path is rendered (path_result.html) and optionally visualized (graph.html) with supporting JSON.


## 2) Current behavior and constraints

- BFS focuses on the smallest number of “hops” between nodes (e.g., shared track/artist collaborations).
- Weighted/Dijkstra incorporates edge weights (e.g., popularity, recency, audio similarity) to prefer higher-quality paths.
- Spotify API usage includes pagination limits and rate limits; code fetches batches and may do multiple requests per layer of BFS.
- Session/token handling supports refreshing but likely keeps tokens in memory or simple storage; scalability is not primary.
- Tests cover search strategies and parsing but may not fully simulate API rate limiting and graph scale.


## 3) Observed technical risks and likely bottlenecks

1) API call explosion during search
   - BFS/weighted search that expands neighbors naïvely will trigger many Spotify API calls per frontier layer (e.g., get artist’s tracks, collaborators, related artists). With branching factor b and depth d, calls can grow O(b^d).
   - Risk: Rate limiting (HTTP 429), high latency, and hitting daily quotas.

2) Redundant fetching and lack of caching
   - The same artists/tracks may be re-fetched across layers or across different user requests.
   - Without a memo/TTL cache, duplicate API calls waste time and quota.

3) Inefficient data structures for frontier/visited
   - If visited structures are not using compact hashed IDs, lookups could be slower and memory heavier, especially for large search depths.

4) Over-eager template rendering / blocking requests
   - If long searches are done synchronously in the request goroutine, timeouts can occur and the UI will block.

5) Token storage and concurrency
   - Storing tokens in memory without per-user isolation or proper locking can lead to data races across concurrent requests.

6) Graph construction policy
   - If edges are constructed on-the-fly from heterogeneous endpoints (tracks, artists, related artists, features), the node/edge semantics may be inconsistent, causing path quality issues.

7) Error handling and retries
   - Spotify transient errors and 429 backoffs must be retried with exponential backoff and respect Retry-After headers.

8) Pagination handling
   - Spotify endpoints return pages; failure to cap, batch, and early-stop on heuristic limits can balloon work.

9) Testing gaps
   - Heavy reliance on tests for pure logic; limited integration tests with mocked Spotify responses for large graphs and rate limiting scenarios.


## 4) Optimization opportunities and proposed fixes

A) Introduce layered caching with TTL
- Add an in-process cache (LRU/TTL) for: getArtist(id), getTrack(id), getArtists(ids batch), getTracks(ids batch), artist’s tracks, track’s artists, related artists, audio features.
- Cache key: method + normalized params (e.g., artist:{id}:tracks?limit=50). Value: JSON or typed structs + timestamp.
- TTL suggestions: 1–6 hours for mostly static data (artist metadata), 5–15 minutes for dynamic endpoints (currently playing), and 24 hours for audio features.
- Benefits: Reduce duplicate API calls across a search and across requests; improves latency and resilience.

B) Batch requests and prefer bulk endpoints
- Where Spotify supports batching (e.g., Get Several Tracks/Artists/Audio Features), accumulate pending IDs and fetch in chunks of 50–100.
- Refactor search expansion to gather neighbor IDs first, then perform batched fetches per layer.

C) Rate limit and backoff middleware
- Wrap all API calls with a client that:
  - Observes 429 with Retry-After and sleeps appropriately.
  - Uses token bucket/leaky bucket per app and per user.
  - Has exponential backoff with jitter for 5xx.
- Centralize in the Spotify client to avoid duplicated logic.

D) Frontier expansion heuristics
- Early cut-offs:
  - Max nodes per layer (cap branching factor) based on a scoring function (e.g., popularity + similarity).
  - Depth limit configurable by user or environment.
- Scoring:
  - Rank neighbors using combined metrics (popularity, audio similarity cosine distance on feature vectors, recency) to push promising nodes to the front of the queue.
- Deduplicate neighbors within the same layer to prevent redundant enqueues.

E) Precomputed or ephemeral graph snapshot
- Option 1: Ephemeral in-memory graph during a single search, reusing nodes across expansions within that request.
- Option 2: Warm graph cache persisted to disk or Redis keyed by seed inputs and heuristic options.
- Option 3: Background job to prefetch popular artists/tracks and their collaborations nightly to accelerate common queries.

F) Concurrency model improvements
- Use worker pools for API fetching while bounding concurrency to avoid overwhelming rate limits (e.g., 5–10 workers per user). Fan-out for each frontier layer with context cancellation.
- Use contexts with deadlines for each request; cancel downstream goroutines on timeout or when path found.

G) Memory-efficient visited sets and path reconstruction
- Store visited as map[string]struct{} keyed by Spotify ID.
- Parent pointers map[string]string to reconstruct paths. Consider bit-packed frontiers or roaring bitmaps if IDs mapped to ints via an ID index during a single search.

H) Stable node semantics
- Decide the node type per algorithm run: artist graph only, or a bipartite artist-track graph. Avoid mixing ad hoc relations in the same search to keep path meaning clear.
- Provide a toggle in the UI to select graph type and explain differences.

I) Improved error handling
- Central response wrapper that returns typed errors with context: endpoint, params, status code, retry-after.
- Retries with exponential backoff and circuit breaker when Spotify is down.
- Graceful degradation in UI: show partial paths or “best effort so far” with a cancel button.

J) Persisted tokens and thread-safe store
- Store tokens per user securely (e.g., file DB, SQLite, or Redis). Ensure locking around read/write and refresh flows.
- Encrypt at rest if writing to disk; rotate refresh tokens when provided.

K) Observability
- Structured logging with correlation IDs per request.
- Metrics: requests, API calls per endpoint, cache hit rate, 429 count, average search depth, path found/timeout ratio.
- Tracing: annotate Spotify API spans and BFS layers with sizes and durations.

L) Testing enhancements
- Add integration tests with a mocked Spotify API server responding with paginated data, 429s, 5xx, and randomized delays.
- Property tests for BFS/weighted search to validate path optimality under randomized graphs.
- Benchmarks: measure expansions/sec, API calls per path, memory footprint.

M) UI/UX performance
- Convert long-running search to async job:
  - Request creates a job ID, returns immediately.
  - Client polls or subscribes via SSE/WebSocket for progress and final path.
  - Show progressive frontier stats and allow cancel.
- Graph visualization: send compact nodes/edges; lazy-load details on click.

N) Configuration and safety limits
- Externalize limits (max depth, max nodes per layer, API concurrency, cache sizes) via config file/env vars.
- Provide per-user quotas and safeguards to avoid abusive searches.


## 5) Concrete refactor plan (phased)

Phase 1 — Stability and efficiency
- Implement a thin Spotify client wrapper with:
  - Retries/backoff, 429 handling, and context support.
  - LRU+TTL cache with metrics.
  - Batch endpoints helpers.
- Refactor BFS/weighted expansion to batch fetch neighbor data per layer and deduplicate.
- Add visited/parent pointer maps if not present; ensure constant-time checks.

Phase 2 — Concurrency and async UX
- Introduce worker pool for neighbor fetching with bounded concurrency.
- Convert search handler to create async jobs and return job IDs; add status endpoint and polling in templates.

Phase 3 — Observability and configuration
- Add structured logging, metrics, and tracing hooks around each API call and BFS layer.
- Add configuration file/env with sane defaults for limits and concurrency.

Phase 4 — Graph semantics and quality
- Provide option for artist-only vs artist-track bipartite graph. Tune weighting function with configurable weights. Add audio-feature similarity into weights.

Phase 5 — Persistence and scale
- Move token and cache storage to a small embedded DB (SQLite/Badger) or Redis if already available. Add eviction policies and per-user isolation.


## 6) Quick wins
- Add HTTP client with timeout and retry for all Spotify calls.
- Add simple in-memory map cache with TTL for getArtist/getTrack and related lists.
- Cap BFS depth and branching factor via config; expose defaults in README.
- De-duplicate neighbor IDs per layer before any API fetch.
- Pre-sort candidates by popularity to explore likely connectors first.


## 7) Non-functional
- Security: Store tokens securely and never log access tokens. Use PKCE for OAuth if possible.
- Privacy: Avoid persisting personal playback history unless necessary; document retention.
- Compliance: Respect Spotify’s Developer Terms for caching and data retention.


## 8) Acceptance criteria for optimizations
- 50% reduction in Spotify API calls per average successful path compared to baseline.
- 99th percentile search latency reduced by 40% with same inputs.
- Zero unhandled 429s; all 429s retried respecting Retry-After.
- Cache hit rate > 60% on repeated searches with overlapping neighborhoods.
- No data races under concurrent search load (verified with -race and tests).


## 9) References in repo
- Algorithms: bfs.go, dijkstras.go, sixDegrees/weightedSearch.go
- Spotify API: spotify/spotify.go
- HTTP handlers: main/http_search.go, main/auth.go, main/main.go
- Storage: main/store.go, store.go
- Templates: templates/*.html
- Docs: docs/PDR.md, docs/PDR_frontend.md


End of document.